{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for this notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import VarianceThreshold to removes all low-variance features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** thing to remember is that we should perform feature selection with our training data only. Always always remember that whenever we are doing these \"preparing data for Machine Learning steps\", or in other words Feature Engineering, we should always keep this in mind with all operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constant Features\n",
    "Constant features do not provide any information useful for further analysis or predicting the target variable. These features provide only a single value for all of the observations in the dataset. Therefore, we can remove them from the dataset.\n",
    "\n",
    "We will be working with the subset of Santander Bank dataset $^{1}$ (30 000 rows), which contain anonymized features to predict customer satisfaction regarding their experience with the bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subset dataset called 'subset_santander.csv' and store it to variable data\n",
    "data = pd.read_csv('Data/subset_santander.csv')\n",
    "# Print the shape of the dataframe and get the first 10 rows\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, feature selection should be performed only on our training data to avoid overfitting. Let's split our dataset and drop our TARGET feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels = ['TARGET'], axis = 1),\n",
    "    data['TARGET'],\n",
    "    test_size = 0.3,\n",
    "    random_state = 42)\n",
    "\n",
    "# Get the shape of training and testing set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 370 features for now in our sets. We can now look at whether there are some constant features in X_train set using `NumPy's .var()` function, which computes the variance along with columns. Within this function we can specify argument `ddof = 1`, for more information here is the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.var.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features that have the variance equal to zero\n",
    "# Optional: Specify ddof = 1 within the `.var()` function \n",
    "our_constant_features = X_train.loc[:, X_train.var(ddof = 1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our_constant_features\n",
    "our_constant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 64 features with zero variance, which will be removed from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features from X_train, do not forget specify argument inplace = True\n",
    "X_train.drop(labels= our_constant_features, axis=1, inplace=True)\n",
    "# Remove constant features from X_test, do not forget specify argument inplace = True\n",
    "X_test.drop(labels= our_constant_features, axis=1, inplace=True)\n",
    "\n",
    "# Get the shape after removing constant features\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quasi-constant Features\n",
    "\n",
    "Quasi-constant features have very low variance (or close to 0) and contain less information that is not useful for us. These approximately constant features won't help the ML model's performance. Therefore we should consider removing them. \n",
    "\n",
    "We could filter quasi-constant features with `pandas`  in a similar way as we did with constant features, with one difference - we would set a specific threshold.\n",
    "Nevertheless, now we'll leave `pandas` and enter `scikit learn` that offers a more convenient way to find quasi-constant features. \n",
    "\n",
    "In the `sklearn.feature_selection module` we can find feature selector called `VarianceThreshold()`, which finds all features with low variance (based on a specified threshold) and remove them. You can find more information about this selector [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html).\n",
    "\n",
    "As a first step, we define our selector for quasi-constant features, where we set the threshold = 0.01. In other words, this is the minimum value of the variance we want to have in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VarianceThreshold() object and specify parameter threshold = 0.01\n",
    "our_selector = VarianceThreshold(threshold = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit 'X_train' data with 'our_selector' that find quasi-constant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X_train with 'our_selector'\n",
    "our_selector.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `get_support()` method, we can get all of the features we want to keep along with their names. This 'mask' we will use later to assign columns names.\n",
    "\n",
    "*Note: You might wonder why we are all of a sudden saving feature names as features_to_keep variable. Didn't we say that almighty scikit learn will always save the necessary state inside of the fitted transformer? Well it does, we only do it for our convience so that we can later on go back from the nameless numpy to a nice dataframe with all the column names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mask of features we want to keep in the dataset\n",
    "features_to_keep = X_train.columns[our_selector.get_support()]\n",
    "# Print the length\n",
    "print('The number of features that will be kept: {}'.format(len(features_to_keep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is transforming our X_train and X_test data using our_selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and X_test = in this step, the quasi-constant featues will be finally removed\n",
    "X_train = our_selector.transform(X_train)\n",
    "X_test = our_selector.transform(X_test)\n",
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train is `numpy.ndarray` object after transforming and needs to be transformed into `pandas DataFrame` again. Here, we use our created 'features_to_keep' variable to assign column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train to pandas DataFrame\n",
    "X_train= pd.DataFrame(X_train)\n",
    "# Using '.columns' attribute assign column names\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "# Convert X_test to pandas DataFrame\n",
    "X_test= pd.DataFrame(X_test)\n",
    "# Using '.columns' attribute assign column names\n",
    "X_test.columns = features_to_keep\n",
    "\n",
    "# Get the first 5 rows of X_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Duplicated Features (READ-ONLY)\n",
    "Duplicated features are 2 the same features, thus not providing any useful or new information for improving the model's performance.\n",
    "\n",
    "To better understand how duplicated features can be treated using `pandas`, we create new DataFrame. We've already seen `duplicated()` function, that returns boolean Series denoting duplicate rows. To identify duplicated features, we have to first transpose our data frame, in other words, we swap the rows and columns. More information [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html).\n",
    "\n",
    "You might wonder again, why are we not using the almighty scikit? The reason is that duplicated features should be already addressed within data integration and preprocessing. You might remember that these are the earliest stages. The reason is that duplicated values usually occur when we are merging data from various sources. **It is of course then not the highest priority for scikit developers to implement a specific transformer for this**.\n",
    "\n",
    "Finally, we are doing an ugly operation of swapping rows and columns to make use of Pandas functionality to make this operation as easy as possible. Yes, we are advising to do this with a small dataset such as ours, but think of us in this case as a slightly evil witch. If you have a *big* dataset, for example, counted in TBs, you will most likely not be able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS :)\n",
    "\n",
    "Image('Image/witch.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to create new DataFrame\n",
    "our_data  =[(1, 'Colin Trevorrow', 124, 150 ,'Colin Trevorrow', 150, 'Jurassic World'),\n",
    "           (2, 'George Miller', 120, 55, 'George Miller', 55, 'Mad Max: Fury Road'),\n",
    "           (3, 'Robert Schwentke', 119, 112, 'Robert Schwentke', 112, 'Insurgent'),\n",
    "           (4, 'J.J. Abrams', 136, 220,'J.J. Abrams', 220, 'Star Wars: The Force Awakens'),\n",
    "           (5, 'James Wan', 137, 154, 'James Wan', 154, 'Furious 7'),\n",
    "           (6, 'Bruce Brown', 95, 25, 'Bruce Brown', 25, 'The Endless Summer'),\n",
    "           (7, 'Woody Allen', 80, 15, 'Woody Allen', 15, 'What`s Up, Tiger Lily?'),\n",
    "           (8, 'James Cameron', 162, 180, 'James Cameron', 180, 'Avatar'),\n",
    "           (9, 'Carl Tibbetts', 74, 44, 'Carl Tibbetts', 44, 'Black Mirror: White Christmas'),\n",
    "           (10, 'Harold P. Warren', 74, 8, 'Harold P. Warren', 8, 'Manos: The Hands of Fate')]\n",
    "\n",
    "movies = pd.DataFrame(our_data, columns= ['id', 'director', 'runtime','total_votes', 'name', 'number_of_votes', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print movies DataFrame\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of movies\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, movies DataFrame contains 10 rows and 7 features. Now we use `.transpose()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose movies and store it to variable movies_transpose\n",
    "movies_transpose = movies.transpose()\n",
    "movies_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of movies_transpose\n",
    "movies_transpose.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transposing, there are 7 rows (features) and 10 columns in movies_transpose.\n",
    "\n",
    "Now we apply chained `duplicated().sum()` function on movies_transpose that give us the total number of duplicated rows (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of duplicated rows (features)\n",
    "movies_transpose.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 duplicated rows (features), containing the same observations. Using `.drop_duplicates()`, we can drop duplicated rows. By setting `keep = 'first'` parameter, we determine which duplicated row we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and store the result in variable unique_features\n",
    "unique_features = movies_transpose.drop_duplicates(keep = 'first').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get duplicated features and store the result in variable duplicated_feature\n",
    "duplicated_features = [column for column in movies.columns if column not in unique_features]\n",
    "duplicated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated features from the original DataFrame\n",
    "movies.drop(labels = duplicated_features, axis = 1, inplace = True)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this approach is not computationally and memory-efficient if you have a really large DataFrame with thousands of rows. As `scikit learn` does not offer a method to handle duplicated features, we need to create some function for this purpose. Then we drop duplicated features using `pandas .drop()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list for duplicated features\n",
    "features_duplicates = []\n",
    "# Create for loop for iterating over the range of columns from X_train set\n",
    "for col in range(len(X_train.columns)):\n",
    "     \n",
    "    column_1 = X_train.columns[col]\n",
    "    # Find duplicated features by comparing columns using .equals\n",
    "    for column_2 in X_train.columns[col + 1:]:\n",
    "        if X_train[column_1].equals(X_train[column_2]):\n",
    "            features_duplicates.append(column_2)\n",
    "            \n",
    "len(features_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated features from X_train and X_test\n",
    "X_train.drop(labels = features_duplicates, axis = 1, inplace = True)\n",
    "X_test.drop(labels = features_duplicates, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit learn` module offers many methods such as selecting features based on their importance, but we do not go there. You can find these methods in the [documentation](https://scikit-learn.org/stable/modules/feature_selection.html). Now we'll look at the correlation between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation\n",
    "The features that have high correlation have almost the same effect on the target feature. We can visualize relationships between features using `.corr()` method to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlation among the columns and store it in variable correlation_matrix\n",
    "correlation_matrix = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix \n",
    "plt.figure(figsize=(11,11))\n",
    "sns.heatmap(correlation_matrix, cmap = 'Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the highly correlated features using function based on the correlation coefficient above the threshold of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    # Create set for correlated columns\n",
    "    corelated_cols = set()  \n",
    "    # Compute correlation \n",
    "    corr_matrix = dataset.corr()\n",
    "    for c in range(len(corr_matrix.columns)):\n",
    "        for j in range(c):\n",
    "            # We take absolute correlation coefficient value \n",
    "            # If this abs values are above threshold...\n",
    "            if abs(corr_matrix.iloc[c, j]) > threshold: \n",
    "                # ...Get the name of column\n",
    "                colname = corr_matrix.columns[c]\n",
    "                corelated_cols.add(colname)\n",
    "    return corelated_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use correlation function on X_train with threshold 0.8\n",
    "corr_features_to_drop = correlation(X_train, 0.8)\n",
    "len(set(corr_features_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop correlated features from X_train and X_test\n",
    "X_train.drop(labels = corr_features_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(labels = corr_features_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder, where the heck is scikit again? Why doesn't it have a cool transformer that will drop us highly correlated features? The reason is simple. Dropping highly correlated features goes more to the dimension of Statistical Learning. What the philosophy of Machine Learning would do, would be so called Principal Component Analysis. An efficient way to get rid of cross correlation, but we lose the \"meaning\" of original features. You will learn about this technique when we come to Unsupervised Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK\n",
    "\n",
    "You will be using an altered dataset containing variants of the Portuguese 'Vinho Verde' wine $^{2}$. The features provide information about wine samples recorded based on physicochemical tests. There is also the target feature that denotes the quality score of the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset 'wine_quality.csv' and store it to variable wine\n",
    "wine = pd.read_csv('Data/quality_of_wine.csv', sep = ',', )\n",
    "# Get the first 10 rows\n",
    "wine.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataframe's datatypes\n",
    "wine.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several numerical features are stored as float or integer, and one feature is stored as string data in our dataset. \n",
    "\n",
    "These numerical variables can be used to predict the quality of wine samples. So **'quality'** column is our **target feature**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset 'wine' into training and testing set\n",
    "# Store it in variables: X_training, X_testing, y_training, y_testing\n",
    "# Drop target feature 'quality'\n",
    "# Set test_size = 0.3 and random_state = 42\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(wine.drop(labels = ['quality'], axis=1), wine['quality'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Get the shape of training and testing set\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, there is one non-numerical variable, and so 'type'. Let's look at the unique values of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of 'type' column in X_training and X_testing sets\n",
    "print(X_training['type'].unique())\n",
    "print(X_testing['type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets can also contain this type of constant feature stored as a string and have only 1 unique value/category. As this variable is not really helpful, we will drop it from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Remove constant feature from X_training using '.drop()', do not forget specify argument inplace = True\n",
    "\n",
    "# TASK >>>> Remove constant feature from X_testing using '.drop()', do not forget specify argument inplace = True\n",
    "\n",
    "\n",
    "# Get the shape of X_training and X_testing sets\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to select only those features that have the variance above the threshold = 0.01. Again, we will find quasi-constant features using `scikit learn's VarianceThreshold` as we did in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Define VarianceThreshold() object, specify parameter threshold = 0.01 and store it in variable 'selector'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit X_training set with 'selector'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mask of features we want to keep in the dataset and store it in variable 'features_we_keep'\n",
    "features_we_keep = X_training.columns[selector.get_support()]\n",
    "# Print the length of the variable features_we_keep\n",
    "print('The number of features that will be kept: {}'.format(len(features_we_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the quasi-constant features that are meant to drop using for loop \n",
    "for column in X_training.columns:\n",
    "    if column not in features_we_keep:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform X_training \n",
    "\n",
    "# TASK >>>> Transform X_testing\n",
    "\n",
    "\n",
    "# Get the shape of X_training and X_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_training to pandas DataFrame\n",
    "X_training = pd.DataFrame(X_training)\n",
    "# Using '.columns' attribute assign column names\n",
    "X_training.columns = features_we_keep\n",
    "\n",
    "# Convert X_testing to pandas DataFrame\n",
    "X_testing = pd.DataFrame(X_training)\n",
    "# Using '.columns' attribute assign column names\n",
    "X_testing.columns = features_we_keep\n",
    "\n",
    "# Get the first 10 rows of X_train\n",
    "X_training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find whether our dataset contains duplicated features. You can copy-paste for loop we've already used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated features in X_training set\n",
    "features_duplicates = []\n",
    "for col in range(len(X_training.columns)):\n",
    "     \n",
    "    column_1 = X_training.columns[col]\n",
    "    \n",
    "    for column_2 in X_training.columns[col + 1:]:\n",
    "        if X_training[column_1].equals(X_training[column_2]):\n",
    "            features_duplicates.append(column_2)\n",
    "            \n",
    "len(features_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features names\n",
    "features_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Drop these duplicated features from X_training and X_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of X_training and X_testing\n",
    "X_training.shape, X_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix\n",
    "\n",
    "Data sources:\n",
    "\n",
    "$^{1}$ Santander dataset: https://www.kaggle.com/c/santander-customer-satisfaction/data\n",
    "\n",
    "$^{2}$ Wine quality dataset: https://archive.ics.uci.edu/ml/datasets/wine+quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "834px",
    "left": "0px",
    "right": "20px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
