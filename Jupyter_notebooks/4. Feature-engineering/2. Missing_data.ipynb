{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas, numpy, seaborn and matplotlib libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import train_test_split to separate train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import MissingIndicator and SimpleImputer from impute module\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters and the style for plotting\n",
    "params = {'figure.figsize':(12,8),\n",
    "         'axes.labelsize':13,\n",
    "         'axes.titlesize':16,\n",
    "         'xtick.labelsize':11,\n",
    "         'ytick.labelsize':11\n",
    "         }\n",
    "plt.rcParams.update(params)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Titanic dataset to explore missing data in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset 'Data/titanic_data.csv' and store it in variable data\n",
    "data = pd.read_csv('Data/titanic_data.csv')\n",
    "# Get the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First look at the missing values\n",
    "\n",
    "We can use Pandas chained `isnull().sum()` function to detect missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missing values using\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that three columns contain missing values: 'Age', 'Cabin' and 'Embarked'. If we want to compute the proportion of missing values, we can use `.mean()` function and plot the proportion using barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the proportion of missing values\n",
    "percentage = data.isnull().mean()*100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barchart\n",
    "percentage.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to visualize the location of missing values, we can use `seaborn's heatmap` that tells us where the missing values occur. We set parameter `cbar = False` as the color bar does not need to be drawn.\n",
    "\n",
    "Such a visualization has a benefit which people usually do not realize. Imagine that you just produce sums or in other words amounts of missing values in the dataset. Remember that descriptive statistics might reveal less than what visualisation does. This is also true for missing values. You might be able to spot, for example, **that missing values in two columns have similar or same pattern**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize only those three columns that contain missing values\n",
    "data_copy = data[['Age','Cabin','Embarked']]\n",
    "sns.heatmap(data_copy.isnull(), cbar = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concepts of missing values\n",
    "\n",
    "According to Rubin's theory $^{1}$, every data point has some probability of being missing in the dataset. The process that governs these probabilities is called **the missing data mechanism**. \n",
    "\n",
    "## 2.1 MNAR: Missing data Not At Random\n",
    "\n",
    "MNAR means that the probability of being missing varies for reasons that are unknown to us. Let's look at the columns 'Age' and 'Cabin' in which passengers were traveling. We found out that the column 'Cabin' contains approximately 77% missing values, the column 'Age' almost 20% missing values. \n",
    "\n",
    "The age or cabin could not be established for people who did not survive that night. We assume that survivors were asked for such information. But can we infer this when we look at the data? In this case, we expect that observations with people who did not survive should have more missing values. Let's find out.\n",
    "\n",
    "*Note: Below is a cool functionality of pandas. The method is called query and allows you to really simply subset your data. Of course you could also solve with the traditional functionality which you already learned, I just wanted to make use of the opportunity.*\n",
    "\n",
    "### 2.1.1 Diagnosing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on people who survived\n",
    "survived = data.query('Survived == 1')\n",
    "survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of missing values in column 'Cabin' for people who survived\n",
    "print('The percentage of missing values: {0:.1f} %'.format(survived['Cabin'].isna().mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on people who did not survived\n",
    "not_survived = data.query('Survived == 0')\n",
    "not_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of missing values in column 'Cabin' for people who did not survive\n",
    "print('The percentage of missing values: {0:.1f} %'.format(not_survived['Cabin'].isna().mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output we obtained is the same as we expected. There is more missing values(approximately 87.6%) for people who did not survive compared to the survivals (60.2 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 >>>> Now it's your turn to explore the column 'Age' in the same way \n",
    "#             and think about whether the values are missing not at random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we've filled in missing values using Pandas `fillna() method`. We can specify a scalar value, method such as backward fill ('bfill'), or forward fill ('ffill'), or statistic such as mean, median, or mode of the particular column within this method. If we want to replace missing data with 'bfill' method or 'ffill' method and the previous or the next value is not present, the missing values remain present after the imputation. Also, be careful when filling in missing values with the mean if your data have outliers because the mean is affected by them.\n",
    "\n",
    "This approach of filling missing values should be sufficient if you will use the dataset for simple analysis. However remember what we discussed. As soon as we want to build a robust pipeline, for example, for Machine Learning, we need to be able to save the state. This of course means that the Pandas functionality does not come in handy. We would need to be manually saving the state of *\"mean which should be imputed\"* somewhere. \n",
    "\n",
    "Luckily for us, scikit-learn offers a handy alternative in forms of **missing indicator** and **simple imputer**. Both of these are saving the state so that we can easily make those part of our robust pipeline. Let's now take a look at these two.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "**Simple Imputer and Missing Indicator**\n",
    "\n",
    "`scikit learn` offers transformers for Univariate and Multivariate imputation of missing values. You can read more in the [documentation](https://scikit-learn.org/stable/modules/impute.html). Now we demonstrate the usability of `SimpleImputer()` class from the impute module. You can specify several parameters, such as the placeholder (np.nan) for missing values, the imputation strategy, or the value used to replace missing values. Find more [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
    "\n",
    "\n",
    "\n",
    "**Before we impute the missing values**, it is useful to mark missing values to preserve the information about which values had been missing. We can use `MissingIndicator`, which transforms the dataset into a binary variables indicating the presence of missing values (these binary variables will be added to the original training set). See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) for MissingIndicator.\n",
    "\n",
    "In essence the idea behind missing indicator is that we **preserve extra information** for our model, which is that the value was missing. We are hoping that the model might pick up a pattern herein which we missed, for example when exploring the missing values.\n",
    "\n",
    "Let's split our data into training and testing set, mark missing values, and fill in those using SimpleImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "                                                        'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']], \n",
    "                                                    data['Survived'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42) \n",
    "# Get the shape \n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of missing values\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Missing Indicator\n",
    "We'll use Missing indicator to mark missing values by setting parameters `features = 'missing-only'` (by default). If we want to create binary variables for all features, we set parameter `features = 'all'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MissingIndicator object\n",
    "missing_indicator = MissingIndicator(features = 'missing-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X_train with missing_indicator \n",
    "missing_indicator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with missing values\n",
    "missing_indicator.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.features_` attribute, we get feature names or the indices of features containing missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features names\n",
    "X_train.columns[missing_indicator.features_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the transformation of X_train returns boolean, we create a new variable to store the output. After that, we concatenate it to the original X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using missing_indicator and store the output to X_missing\n",
    "X_train_missing = missing_indicator.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the output\n",
    "X_train_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we concatenate X_train_missing boolean variables to the original X_train. To distinguish which boolean variable belongs to the original feature in X_train, we create new names (otherwise, boolean variables names will be labeled as 0,1 and 2). X_train_missing array needs to be converted using `pd.DataFrame`, since only Series and DataFrame objects are valid within `concat()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column names for boolean variables\n",
    "# Create an empty list to store new names\n",
    "indicator_name = []\n",
    "# Iterate over the features with missing values\n",
    "for column in X_train.columns[missing_indicator.features_]:\n",
    "    column_name = column + '_Missing'\n",
    "    # Append new names to the indicator_name list\n",
    "    indicator_name.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original X_train and X_train_missing along the columns\n",
    "# Reset the index in X_train and convert X_train_missing to pandas DataFrame \n",
    "# Assign new column names stored in indicator_name to columns parameter\n",
    "X_train = pd.concat([X_train.reset_index(), pd.DataFrame(X_train_missing, columns = indicator_name)], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>>> Repeat the process for X_test data\n",
    "# Transform X_test data using missing_indicator and store it to variable X_test_missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 >>>>> Concatenate the original X_test and X_test_missing along the columns in the same way as we did for X_train\n",
    "# Assign it to the original X_test\n",
    "\n",
    "# Display X_test to see the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Simple Imputer\n",
    "Now we'll impute missing values of column Age using Simple Imputer. We specify the placeholder for missing values (np.nan) and strategy = 'mean' (this strategy is by default, so it is okay if you don't explicitly specify it within the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SimpleImputer object for imputing missing values with mean strategy\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit column Age in X_train\n",
    "imputer.fit(X_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the mean value that will be used for imputing we can use .statistics_ attribute\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform column Age in X_train\n",
    "X_train['Age'] = imputer.transform(X_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missing values in column Age to see whether these values have been replaced\n",
    "X_train['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3 >>>>> Repeat the imputing also for colum Age in X_test data\n",
    "\n",
    "# Get the total number of missing values in column Age to see whether these values have been replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MCAR: Missing data Completely At Random \n",
    "\n",
    "When data are missing completely at random, the probability of being missing is the same for all observations in the dataset, i.e., the cause of the missing data is unrelated to the data.\n",
    "\n",
    "Let's take as an example column 'Embarked' and its missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows where the values in 'Embarked' column are missing\n",
    "data[data['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mrs. Stone was traveling in the first class with her maid Miss. Amelie Icard. They occupied the same Cabin B28, but the data about the port of embarkation are missing. But we can not tell if the Embarked variable depends on any other variable. We can also see that these women have survived, so we assume that they were asked for that information. It could happen that this information was lost when this dataset was created. The probability of losing this information is the same for every person on the Titanic. However, it would probably be impossible to prove. \n",
    "\n",
    "For curiosity: You can find out more information about Mrs. Stone and her maid [here](https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html). There is also information about the port of embarkation in this article.  \n",
    "\n",
    "-----\n",
    "\n",
    "We can impute missing values also in case of categorical variables that have values stored as strings. Let's impute missing values of Embarked column in X_train data. We set `strategy = constant` that allows us to specify the `fill_value` used to replace missing values. This can be used with strings or numeric data as well. The second option for strategy is `most_frequent` when the missing values will be replaced using the most frequent column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SimpleImputer object and store it to variable imputer_cat\n",
    "imputer_cat = SimpleImputer(missing_values = np.nan, strategy = 'constant',fill_value = 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit column Embarked in X_train\n",
    "imputer_cat.fit(X_train[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform column Embarked in X_train\n",
    "X_train['Embarked'] = imputer_cat.transform(X_train[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the total number of missing values in column Embarked to see whether these values have been replaced\n",
    "X_train['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 >>>>> Repeat the imputing also for column Embarked in X_test data\n",
    "\n",
    "# Get the total number of missing values in column Embarked to see whether these values have been replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MAR: Missing At Random\n",
    "\n",
    "We can say that the data are missing at random if the probability of being missing is the same only within groups defined by the observed data. An example of this case is when we take a sample from a population. The probability to be included depends on some known property. For example, when placed on a soft surface, a weighing scale may produce more missing values than when placed on a hard surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "In this task, you will be using the Avocado dataset. You will impute numeric missing values in column 'Small Bags' with the median value using Simple Imputer. The second task is to impute missing values in the column 'Region' with the most frequent string value of this column using Simple Imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset 'Data/avocado_missing.csv' and store it to variable avocado\n",
    "avocado = pd.read_csv('Data/avocado_missing.csv')\n",
    "# Print the first 5 rows\n",
    "avocado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Print the total number of missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Create SimpleImputer object and store it in variable imputer_median\n",
    "# Specify that you want to impute median value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit column 'Small Bags' using imputer_median \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median value that will be used to replacing missing values\n",
    "imputer_median.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform column 'Small Bags' using imputer_median\n",
    "# Assign the transformation to avocado['Small Bags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Create SimpleImputer object and store it in variable imputer_freq\n",
    "# Specify that you want to impute the most frequent value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Fit column 'region' using imputer_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the most frequent value that will be used to replacing missing values\n",
    "imputer_freq.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK >>>> Transform column 'region' using imputer_freq\n",
    "# Assign the transformation to avocado['region']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of missing values to see whether the missing values have been replaced\n",
    "avocado.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Read only - Column Transformer\n",
    "\n",
    "Commonly, preparing data for machine learning models often involve several transformations such as imputing missing values, scaling numerical values, or encoding categorical features applied for particular columns. `scikit learn` offers `ColumnTransformer` class that is used to apply different transformers to columns. This Column transformer can be chained with Pipelines along with machine learning model. You can read more about ColumnTransformer [here](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "$^{1}$ Inference and missing data, DONALD B. RUBIN, Biometrika, Volume 63, Issue 3, December 1976, Pages 581–592\n",
    "\n",
    "Data source: \n",
    "\n",
    "Titanic dataset: https://www.kaggle.com/hesh97/titanicdataset-traincsv\n",
    "\n",
    "Data license: CC0: Public Domain\n",
    "\n",
    "Avocado dataset: https://www.kaggle.com/neuromusic/avocado-prices\n",
    "\n",
    "Data license: Database: Open Database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
