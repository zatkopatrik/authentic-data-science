{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Core Idea of Scikit\n",
    "\n",
    "Remember that our modeling pipeline will consist of three major areas before we get to the creation of the actual model:\n",
    "\n",
    "- Data Preprocessing\n",
    "- Data Visualization (let's not care about this for now, as it usually is not part of the core pipeline)\n",
    "- Preparing Data for Machine Learning (so called Feature Engineering)\n",
    "\n",
    "Within *Data Preprocessing*, things are simple. If we once define that there is a column \"city_name\" from which all whitespaces should be stripped, we can easily replicate this operation whenever new data comes into out pipeline on which we should predict. We simply...you might have guessed it...strip all of the whitespaces from that column. **We do not need to save the state to replicate the operation usually within preprocessing.**\n",
    "\n",
    "Things are a bit more tricky within *Preparing Data for ML*. The operations here are usually **not replicable, unless we save certain state of that operation**. For example, if we have decided to impute instead of missing values in our training data a median, we want to impute the same value for all new data that come in. We cannot calculate the median again, we need to reuse it. Each of these operations will have a different state to be saved. Luckily for us, we do not need to learn and care for saving of these states. We just use scikit-learn, which will really simiplify this job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and predicting: estimator basics\n",
    "If you thing about fitting of Machine Learning model, it is a form of transformation right? We are transforming all independent features, into a single target feature. In order for us to replicate this operation, we need to save the state, which in this case will be the model itself. Here is where the cool idea of saving the state originated...\n",
    "\n",
    "Here is a simple example where we fit a RandomForestClassifier to some very basic data. In the cell below we only fit the model. The clf hence contains nothing but the model - or in order words the desired state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "X = [[ 1,  2,  3],  # 2 samples, 3 features\n",
    "     [11, 12, 13]]\n",
    "y = [0, 1]  # classes of each sample\n",
    "clf.fit(X, y)\n",
    "RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the estimator is fitted, it can be used for predicting target values of new data. You donâ€™t need to re-train the estimator. In this case do not get confused by the fact that we are again using X. We did not alter this data before, we have only fitted the model (saved the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)  # predict classes of the training data\n",
    "\n",
    "clf.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and pre-processors\n",
    "\n",
    "The same idea of saving the state has been implemented also for other functionalities that scikit offers. In this case, do not get confused, they refer to \"Preparing Data for ML\" as \"Preprocessing\".\n",
    "\n",
    "Take a look below. We decided to transform our data using a standard scaler. We again at first only fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[0, 15],\n",
    "     [1, -10]]\n",
    "fitted_scaler = StandardScaler().fit(X)\n",
    "fitted_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the transformer (saved state) to replicate the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/getting_started.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
