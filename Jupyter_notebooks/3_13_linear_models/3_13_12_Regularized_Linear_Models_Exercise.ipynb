{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We load boston data from sklearn.datasets and split to train and test data. Additionally, we generate polynomial features of the second degree. We will work further with x_train_poly, y_train, x_test_poly and y_test. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()\n",
    "\n",
    "xd, yd = data.data, data.target\n",
    "x = pd.DataFrame(xd, columns = data.feature_names)\n",
    "y = pd.DataFrame(yd, columns = [\"target\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "x_train_poly = poly.fit_transform(X_train)\n",
    "x_test_poly = poly.transform(X_test)\n",
    "poly_names = poly.get_feature_names()\n",
    "\n",
    "names_dict = {'x0': X_train.columns[0],\n",
    "             'x1': X_train.columns[1],\n",
    "             'x2': X_train.columns[2],\n",
    "             'x3': X_train.columns[3],\n",
    "             'x4': X_train.columns[4],\n",
    "             'x5': X_train.columns[5],\n",
    "             'x6': X_train.columns[6],\n",
    "             'x7': X_train.columns[7],\n",
    "             'x8': X_train.columns[8],\n",
    "             'x9': X_train.columns[9],\n",
    "             'x10': X_train.columns[10],\n",
    "             'x11': X_train.columns[11],\n",
    "             'x12': X_train.columns[12]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- how many features are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further use the user defined function plot_coef that takes as input coefficients as output of the fitted model. It plots the coefficient values and calculates average. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef(lr_coef):\n",
    "    '''\n",
    "    The function plots coefficients' values from the linear model.\n",
    "    --------\n",
    "    params:\n",
    "        lr_coef: coefficients as they are returned from the classifier's attributes\n",
    "    '''\n",
    "    lr_coef = lr_coef.reshape(-1,1)\n",
    "    print(f'AVG coef value: {np.mean(lr_coef)}')\n",
    "    plt.plot(lr_coef)\n",
    "    plt.title(\"Coefficients' values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit linear regression without regularization\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- instantiate linear regression under the name lr\n",
    "- fit lr to x_train_poly, y_train \n",
    "- predict with lr on x_train_poly and store the results to y_hat_train\n",
    "- predict with lr on x_test_poly and store the results to y_hat_test\n",
    "- return RMSE for y_hat_train as well as for y_hat_test. \n",
    "\n",
    "How do you interpret the difference in performance of the model on train and on test dataset? Can you tell if the model overfits/underfits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = ...\n",
    "...\n",
    "\n",
    "y_hat_train = ...\n",
    "y_hat_test = ...\n",
    "\n",
    "rmse_train = ...\n",
    "rmse_test = ...\n",
    "print(f\"RMSE train: {rmse_train}\")\n",
    "print(f\"RMSE test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is almost twice as big for a test set than for a train set. It suggests an overfitting and a poor generalization power of the model.\n",
    "\n",
    "\n",
    "- use function plot_coef on coefficients of the fitted model to see the values of the coefficients and the average value of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in a combination with the error values on train and test suggest that we deal here with overfitting of the model on the given set of polynomial features. We should therefore use regularization. \n",
    "\n",
    "## Standardization\n",
    "\n",
    "Before fitting any regularized model scaling of features is crucial. Otherwise the regularization would not be fair to features of different scales. Regularized linear models assume that the inputs to the model have zero mean and a variance in the same magnitude. StandarScaler() deducts mean and divides by a standard deviation. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "- instantiate StandardScaler() under the name scaler\n",
    "- fit_transform x_train_poly with a scaler and store the result into x_train_scaled\n",
    "- once the scaler is fit to x_train_poly you can directly transform x_test_poly with the scaler and store into variable X_test_scaled. You never want to fit on a test sample not to leak information from test data. Test data serves only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ...\n",
    "X_train_scaled = ...\n",
    "X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you applied standardization correctly you should see on the bottom chart distributions of all the features concentrated around zero with similar ranges of deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title('Original polynomial features')\n",
    "plt.boxplot(x_train_poly)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Scaled features')\n",
    "plt.boxplot(X_train_scaled)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "### Exercise\n",
    "- instantiate Lasso regression under the name lr_l\n",
    "- fit the model to X_train_scaled, y_train\n",
    "- predict on X_train_scaled and X_test_scaled and store the predictions to y_hat_train and y_hat_test respectively\n",
    "- Did the overfit change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_l = ...\n",
    "...\n",
    "\n",
    "y_hat_train = ...\n",
    "y_hat_test = ...\n",
    "\n",
    "rmse_train = \n",
    "rmse_test = \n",
    "print(f\"RMSE train: {rmse_train}\")\n",
    "print(f\"RMSE test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance seems to be comparable on train and test dataset. Hence, the model's generalization power is good now.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- use plot_coef() on the coefficients of the lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average value of the coefficients is now much smaller. Also many of the coefficients are equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After applying Lasso on polynomial scaled features we remain with {np.sum(lr_l.coef_!=0)} variables')\n",
    "print('\\nThe selected variables are:\\n')\n",
    "[print(val) for val in pd.DataFrame(poly_names)[lr_l.coef_!=0].values];\n",
    "print('\\nmapping from polynomial names to original feature names: ')\n",
    "display(names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- subset from X_train_scaled only those variables that have a non-zero coefficient and store the subset in the variable *x_train_lasso*\n",
    "- do the same selection on X_test_scaled and save to *x_test_lasso*\n",
    "- how many variables are remaining? Check it with the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lasso = ...\n",
    "x_test_lasso = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "We have effectively performed a feature selection with Lasso. Now we will build on it and will use only the selected features in x_train_lasso, x_test_lasso. \n",
    "\n",
    "Let's try different values for the strength of the optimization, alpha. By default it is equal to 1 and it must be a positive value. Larger values specify stronger regularization. Alpha can be set also in Lasso and Elastic Net.\n",
    "\n",
    "### Exercise\n",
    "- fit the ridge regression to x_train_lasso, y_train with the values of alpha 0.001, 0.01, 0.1, 1, 10, 100 to see the effect of the regularization strength.\n",
    "- return RMSE for x_train_lasso for each of the alpha options\n",
    "- select a parameter alpha for which the model has the best RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmses = []\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for alpha in alphas:    \n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- fit the model with the best performance on train data\n",
    "- calculate RMSE on the x_test_lasso for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "...\n",
    "...\n",
    "rmse_test = ...\n",
    "print(f\"RMSE test: {np.round(rmse_test,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSES on the train and the test set are similar.\n",
    "\n",
    "### Exercise\n",
    "- use the function plot_coef on the coefficients from the best model to see the coefficients values with their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net is a combination of Lasso and Ridge which is defined by a parameter l1_ratio. If it is equal to 1 the model is equivalent to Lasso, if it is 0 then it is as if we had Ridge regression. Regularization strength alpha can be defined as in Ridge or Lasso. \n",
    "\n",
    "You can enforce the values of the parameters to be positive with the parameter positive = True. Such an option is also available for Lasso. \n",
    "\n",
    "For all the variations of the linear regression you can enforce it to fit the model without an intercept. This can be done by setting the parameter fit_intercept=False\n",
    "\n",
    "There is an option to scale data by the norm of each feature. If normalization is applied to fitting of the model it is automatically also applied to the predict(). We can use this method instead of standard scaling done at the beginning. \n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- experiment with the parameters of ElasticNet. Fit the model to x_train_lasso, y_train with different set of options, e.g.\n",
    "    - positive=True\n",
    "    - fit_intercept=False\n",
    "    - l1_ratio = 0, 0.5, 1\n",
    "    - alpha = 0.001, 0.01, 0.1, 1, 10, 100\n",
    "    - normalize=True\n",
    "- plot coefficients with a function plot_coef to see the effect on coefficients\n",
    "- return RMSE on train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "rmse_train = ...\n",
    "rmse_test = ...\n",
    "\n",
    "print(f\"RMSE train: {rmse_train}\")\n",
    "print(f\"RMSE test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------\n",
    "Material adapted for RBI internal purposes with full permissions from original authors. Source: https://github.com/zatkopatrik/authentic-data-science"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
